{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mobilenetv2_allcombined_TF_final_withNotes_v2.ipynb","provenance":[{"file_id":"1X8qK0lY4tLXuBa0pIf7GwrjjaxEhpqsF","timestamp":1653671718505},{"file_id":"11i3dkBACvn78Rs5qpt25t56ew3eXDZWr","timestamp":1653605473381},{"file_id":"1dwUaU5Z-3gS6QYe_NfYpwMPc7_k7w3h3","timestamp":1653553251038},{"file_id":"1ME53QFR6FzSH1ir0kMyt_0IdoJBvxjrs","timestamp":1653535681325},{"file_id":"1_CjPz6PkLa-ozKR1itMUEV4e-g3vRO_V","timestamp":1653328869682}],"machine_shape":"hm","background_execution":"on","collapsed_sections":[],"authorship_tag":"ABX9TyNx//HKNuiyPapaqMZfjYFf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## MobileNetV2 model - ALL DATA\n","Creation of MobileNetV2 model for sound recognition <br>\n","Dataset includes Freesound, UrbanSound8k and some custom sounds generated from an iPhone <br>\n","<br>\n","Audio files converted to log-scaled MEL spectrograms. <br>\n","Classification model built from a headless version of MobileNetV2 from (\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"). <br>\n","The MobileNetV2 model was used as a feature extractor and wrapped up as a Keras layer using hub.KerasLayer and frozen so that it will not be modifiable by training.<br> <br>\n","A new model is created by attaching the pre-trained feature extractor model and a new classification head layer. This model is the one compiled and trained. \n"],"metadata":{"id":"IgP_X55njIQR"}},{"cell_type":"markdown","source":["Grant access to Google Drive, where the spectograms are available"],"metadata":{"id":"_7TtBPPGldwT"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n0G8r7hI6giO","executionInfo":{"status":"ok","timestamp":1653926146782,"user_tz":-120,"elapsed":5787,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"c407d552-0178-4227-f609-44583bd97efd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Mounted at /content/gdrive\n","/content/gdrive/My Drive\n"]}],"source":["from google.colab import drive #Only if you are using Google Drive\n","drive.mount('/content/gdrive')\n","drive.mount(\"/content/gdrive\", force_remount=True)\n","%cd /content/gdrive/My\\ Drive/"]},{"cell_type":"markdown","source":["Check contents of Tensorboard logs. Remove if necessary"],"metadata":{"id":"roQBsJlqllNr"}},{"cell_type":"code","source":["#! rm -R /content/gdrive/MyDrive/content/img_dir/logs/all_combined/fit5/*\n","#! ls -al /content/gdrive/MyDrive/content/img_dir/logs/all_combined/fit5/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o8DRkR6PfGkN","executionInfo":{"status":"ok","timestamp":1653912524813,"user_tz":-120,"elapsed":35,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"34ff9d3d-1388-40cd-f469-a682b26ee943"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 0\n"]}]},{"cell_type":"code","source":["!pip install tensorboardcolab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pRSGU2nMT0rX","executionInfo":{"status":"ok","timestamp":1653924399899,"user_tz":-120,"elapsed":5450,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"d8fc461f-aede-4cb0-8831-ddfed49e4213"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboardcolab\n","  Downloading tensorboardcolab-0.0.22.tar.gz (2.5 kB)\n","Building wheels for collected packages: tensorboardcolab\n","  Building wheel for tensorboardcolab (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tensorboardcolab: filename=tensorboardcolab-0.0.22-py3-none-any.whl size=3859 sha256=d15ec2ee864877f834efa013a08cb556a75466bf106bc266130e3d53a3914525\n","  Stored in directory: /root/.cache/pip/wheels/69/4e/4a/1c6c267395cb10edded1050df12af165d3254cfce324e80941\n","Successfully built tensorboardcolab\n","Installing collected packages: tensorboardcolab\n","Successfully installed tensorboardcolab-0.0.22\n"]}]},{"cell_type":"markdown","source":["Load necessary Python modules"],"metadata":{"id":"5lypJZfB6kVs"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.pylab as plabt\n","import numpy as np\n","import os\n","import PIL\n","import time\n","import datetime\n","import PIL.Image as Image\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","\n","#from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n","\n","\n","%load_ext tensorboard"],"metadata":{"id":"WxOOPNbC6msL","executionInfo":{"status":"ok","timestamp":1653926151130,"user_tz":-120,"elapsed":2803,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#file_writer = hub.summary.FileWriter('/content/gdrive/MyDrive/content/img_dir/logs/fit/', sess.graph)\n","#tbc=TensorBoardColab()\n","#summary_writer = tbc.get_writer()\n","#summary_writer = tf.train.SummaryWriter()\n","#summary_writer = tf.summary.create_file_writer(\"/content/gdrive/MyDrive/content/img_dir/logs/freesound/fit/\")\n","#log_dir = \"/content/gdrive/MyDrive/content/img_dir/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","#%tensorboard --logdir logs_dir"],"metadata":{"id":"S6-ZmLQtPOh7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define necessary parameters for the model and location of images in Google Drive"],"metadata":{"id":"fNfrWzfrly4j"}},{"cell_type":"code","source":["batch_size = 32\n","img_height = 224\n","img_width = 224\n","#BATCH_SIZE = 255\n","IMG_SIZE = (224, 224)\n","\n","#train_dir = '/content/gdrive/MyDrive/content/img_dir/combined_data/train'\n","#validation_dir = '/content/gdrive/MyDrive/content/img_dir/combined_data/test/'\n","data_dir = '/content/gdrive/MyDrive/content/img_dir/combined_data/all_labeled/'\n","\n","mobilenet_v2 = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n","\n","feature_extractor_model = mobilenet_v2"],"metadata":{"id":"asCXnsJUGxbZ","executionInfo":{"status":"ok","timestamp":1653926154904,"user_tz":-120,"elapsed":286,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Datasets\n","Define training and validation (test) set using Keras Utility **image_dataset_from_directory** including data augmentation. <br>"],"metadata":{"id":"2g_FJmo0mBVC"}},{"cell_type":"code","source":["train_ds = tf.keras.utils.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)\n","\n","val_ds = tf.keras.utils.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)\n","\n","data_augmentation = tf.keras.Sequential([\n","  tf.keras.layers.RandomFlip('horizontal'),\n","  tf.keras.layers.RandomRotation(0.2),\n","])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dTkYleB8Gx1s","executionInfo":{"status":"ok","timestamp":1653926165425,"user_tz":-120,"elapsed":5170,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"6f3d65d4-0784-4eb1-efca-f002c84b2ef6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 49449 files belonging to 217 classes.\n","Using 39560 files for training.\n","Found 49449 files belonging to 217 classes.\n","Using 9889 files for validation.\n"]}]},{"cell_type":"code","source":["class_names = np.array(train_ds.class_names)\n","print(class_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KaY0_nOHHGMT","executionInfo":{"status":"ok","timestamp":1653926169480,"user_tz":-120,"elapsed":483,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"7045e6f1-338e-4610-a14e-38f70c993761"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['Accelerating_and_revving_and_vroom' 'Accordion' 'Acoustic_guitar'\n"," 'Aircraft' 'Alarm' 'Animal' 'Applause' 'Bark' 'Bass_drum' 'Bass_guitar'\n"," 'Bathtub_filling_or_washing' 'Bell' 'Bicycle' 'Bicycle_bell' 'Bird'\n"," 'Bird_vocalization_and_bird_call_and_bird_song' 'Boat_and_Water_vehicle'\n"," 'Boiling' 'Boom' 'Bowed_string_instrument' 'Brass_instrument' 'Breathing'\n"," 'Burping_and_eructation' 'Bus' 'Buzz' 'Camera' 'Car' 'Car_passing_by'\n"," 'Cat' 'Chatter' 'Cheering' 'Chewing_and_mastication'\n"," 'Chicken_and_rooster' 'Child_speech_and_kid_speaking' 'Chime'\n"," 'Chink_and_clink' 'Chirp_and_tweet' 'Chuckle_and_chortle' 'Church_bell'\n"," 'Clapping' 'Clock' 'Coin_dropping' 'Computer_keyboard' 'Conversation'\n"," 'Cough' 'Cowbell' 'Crack' 'Crackle' 'Crash_cymbal' 'Cricket' 'Crow'\n"," 'Crowd' 'Crumpling_and_crinkling' 'Crushing' 'Crying_and_sobbing'\n"," 'Cupboard_open_or_close' 'Cutlery_and_silverware' 'Cymbal'\n"," 'Dishes_and_pots_and_pans' 'Dog' 'Domestic_sounds_and_home_sounds' 'Door'\n"," 'Doorbell' 'Drawer_open_or_close' 'Drill' 'Drip' 'Drum' 'Drum_kit'\n"," 'Electric_guitar' 'Engine' 'Engine_starting' 'Explosion' 'Fart'\n"," 'Female_singing' 'Female_speech_and_woman_speaking' 'Fill_with_liquid'\n"," 'Finger_snapping' 'Fire' 'Fireworks' 'Fixed-wing_aircraft_and_airplane'\n"," 'Fowl' 'Frog' 'Frying_food' 'Gasp' 'Giggle' 'Glass' 'Glockenspiel' 'Gong'\n"," 'Growling' 'Guitar' 'Gull_and_seagull' 'Gunshot_and_gunfire' 'Gurgling'\n"," 'Hammer' 'Hands' 'Harmonica' 'Harp' 'Hi-hat' 'Hiss' 'Human_group_actions'\n"," 'Human_voice' 'Idling' 'Insect' 'Keyboard_musical' 'Keys_jangling'\n"," 'Knock' 'Laughter' 'Liquid'\n"," 'Livestock_and_farm_animals_and_working_animals' 'Male_singing'\n"," 'Male_speech_and_man_speaking' 'Mallet_percussion'\n"," 'Marimba_and_xylophone' 'Mechanical_fan' 'Mechanisms' 'Meow'\n"," 'Microwave_oven' 'Motor_vehicle_road' 'Motorcycle' 'Musical_instrument'\n"," 'Ocean' 'Organ' 'Packing_tape_and_duct_tape' 'Percussion' 'Piano'\n"," 'Plucked_string_instrument' 'Power_tool' 'Printer' 'Purr'\n"," 'Race_car_and_auto_racing' 'Rain' 'Raindrop' 'Ratchet_and_pawl' 'Rattle'\n"," 'Rattle_instrument' 'Respiratory_sounds' 'Ringtone' 'Run' 'Sawing'\n"," 'Scissors' 'Scratching_performance_technique' 'Screaming' 'Screech'\n"," 'Shatter' 'Shout' 'Sigh' 'Singing' 'Sink_filling_or_washing' 'Siren'\n"," 'Skateboard' 'Slam' 'Sliding_door' 'Snare_drum' 'Sneeze' 'Speech'\n"," 'Speech_synthesizer' 'Splash_and_splatter' 'Squeak' 'Stream' 'Strum'\n"," 'Subway_and_metro_and_underground' 'Tabla' 'Tambourine' 'Tap' 'Tearing'\n"," 'Telephone' 'Thump_and_thud' 'Thunder' 'Thunderstorm' 'Tick' 'Tick-tock'\n"," 'Toilet_flush' 'Tools' 'Traffic_noise_and_roadway_noise' 'Train'\n"," 'Trickle_and_dribble' 'Truck' 'Trumpet' 'Typewriter' 'Typing' 'Vehicle'\n"," 'Vehicle_horn_and_car_horn_and_honking' 'Walk_and_footsteps' 'Water'\n"," 'Water_tap_and_faucet' 'Waves_and_surf' 'Whispering'\n"," 'Whoosh_and_swoosh_and_swish' 'Wild_animals' 'Wind' 'Wind_chime'\n"," 'Wind_instrument_and_woodwind_instrument' 'Wood' 'Writing' 'Yell'\n"," 'Zipper_clothing' 'air_conditioner' 'car_horn' 'children_playing'\n"," 'clapping' 'coughing' 'dog_bark' 'door_bells' 'door_open_close'\n"," 'drilling' 'engine_idling' 'gun_shot' 'jackhammer' 'kitchen_exhaust'\n"," 'microwave beeping' 'microwave_beeping' 'person_walking' 'siren'\n"," 'street_music' 'television _on' 'television_on' 'water_faucet']\n"]}]},{"cell_type":"code","source":["normalization_layer = tf.keras.layers.Rescaling(1./255)\n","train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n","val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n","\n","AUTOTUNE = tf.data.AUTOTUNE\n","train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","for image_batch, labels_batch in train_ds:\n","  print(image_batch.shape)\n","  print(labels_batch.shape)\n","  break"],"metadata":{"id":"KK_xn7iBHTQA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653926172991,"user_tz":-120,"elapsed":1054,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"cf67c22a-1625-4865-ade3-8749a59cb478"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["(32, 224, 224, 3)\n","(32,)\n"]}]},{"cell_type":"markdown","source":["Create the feature extractor by wrapping the pre-trained model as a Keras layer with hub.KerasLayer. Use the trainable=False argument to freeze the variables, so that the training only modifies the new classifier layer:"],"metadata":{"id":"ipmLB2STKaTw"}},{"cell_type":"code","source":["feature_extractor_layer = hub.KerasLayer(\n","    feature_extractor_model,\n","    name=\"MobileNetV2\",\n","    input_shape=(224, 224, 3),\n","    trainable=False)"],"metadata":{"id":"yzwpYcvfKZMn","executionInfo":{"status":"ok","timestamp":1653926177723,"user_tz":-120,"elapsed":1610,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#model.get_layer(index=0).summary()\n","#model.get_layer('keras_layer').summary()"],"metadata":{"id":"1nxVmr2F5yCV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The feature extractor returns a 1280-long vector for each image (the image batch size remains at 32 in this example):"],"metadata":{"id":"sHnHb2lIKiRC"}},{"cell_type":"code","source":["feature_batch = feature_extractor_layer(image_batch)\n","print(feature_batch.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pofnnwrkKZHU","executionInfo":{"status":"ok","timestamp":1653926184055,"user_tz":-120,"elapsed":2590,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"5dbb1472-154b-4dbd-f63b-b20f749754b1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["(32, 1280)\n"]}]},{"cell_type":"markdown","source":["Attach a classification head\n","\n","To complete the model, wrap the feature extractor layer in a tf.keras.Sequential model and add a fully-connected layer for classification"],"metadata":{"id":"hioIYvhiLDHD"}},{"cell_type":"code","source":["num_classes = len(class_names)\n","\n","model = tf.keras.Sequential([\n","  feature_extractor_layer,\n","  tf.keras.layers.Dense(num_classes)\n","])\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ozMXwVMoKZD-","executionInfo":{"status":"ok","timestamp":1653926189635,"user_tz":-120,"elapsed":1018,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"7e187d30-cac5-4ab3-f380-706c6de6c051"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," MobileNetV2 (KerasLayer)    (None, 1280)              2257984   \n","                                                                 \n"," dense (Dense)               (None, 217)               277977    \n","                                                                 \n","=================================================================\n","Total params: 2,535,961\n","Trainable params: 277,977\n","Non-trainable params: 2,257,984\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["#model.get_layer(index=0) #.summary()\n","mylayer=model.get_layer('MobileNetV2') #.summary()\n","print(mylayer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L5pSqvky_lp_","executionInfo":{"status":"ok","timestamp":1653926193543,"user_tz":-120,"elapsed":317,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"e7c08bf8-21ad-4de1-ae80-0c9e250c9254"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["<tensorflow_hub.keras_layer.KerasLayer object at 0x7f5d67172450>\n"]}]},{"cell_type":"code","source":["predictions = model(image_batch)\n","predictions.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dqu4YfqRKZBd","executionInfo":{"status":"ok","timestamp":1653926195919,"user_tz":-120,"elapsed":338,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"0980ecc2-07d9-4759-b05a-6fea95557cd3"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([32, 217])"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["**Train the model**\n","\n","Use Model.compile to configure the training process and add a tf.keras.callbacks.TensorBoard callback to create and store logs:"],"metadata":{"id":"V8_3xxpLLMYJ"}},{"cell_type":"code","source":["# changed default adam optimizer to a very low learning rate\n","#   optimizer=tf.keras.optimizers.Adam(1e-5),\n","model.compile(\n","  optimizer=tf.keras.optimizers.Adam(1e-3),\n","  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","  metrics=['acc'])\n","\n","log_dest = \"/content/gdrive/MyDrive/content/img_dir/logs/all_combined/fit6/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log, histogram_freq=1) # Enable histogram computation for every epoch.\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dest, write_graph=True, write_images=False, histogram_freq=1)\n","\n","    "],"metadata":{"id":"IKODRoJPKwlA","executionInfo":{"status":"ok","timestamp":1653926201877,"user_tz":-120,"elapsed":522,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["! rm -R /content/gdrive/MyDrive/content/img_dir/logs/all_combined/fit6/*\n","! ls -lrt /content/gdrive/MyDrive/content/img_dir/logs/all_combined/fit6"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DiYwy-L4rKFP","executionInfo":{"status":"ok","timestamp":1653926077553,"user_tz":-120,"elapsed":997,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"d530c83f-913c-4817-ecb5-a2089cc92eb0"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["total 0\n"]}]},{"cell_type":"markdown","source":["Now use the Model.fit method to train the model.\n","\n","To visualize the training progress in TensorBoard later, create and store logs an a TensorBoard callback."],"metadata":{"id":"tjAvohtnL6A0"}},{"cell_type":"code","source":["#log = \"/content/gdrive/MyDrive/content/img_dir/logs/all_combined/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","#logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log, histogram_freq=1) # Enable histogram computation for every epoch."],"metadata":{"id":"sGxB60H_NFDd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_EPOCHS = 30\n","\n","history = model.fit(train_ds,\n","                    validation_data=val_ds,\n","                    epochs=NUM_EPOCHS,\n","                    callbacks=tensorboard_callback)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZdMbhBBQ_mH","outputId":"9b7ba323-c48a-4717-9831-a81ba517c9a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n"," 964/1237 [======================>.......] - ETA: 1:33 - loss: 3.3856 - acc: 0.2884"]}]},{"cell_type":"code","source":["#model = keras.models.load_model('path/to/location')"],"metadata":{"id":"zSEMr7CcbUMj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save('/content/gdrive/MyDrive/content/models/mobilenetv2_all_combined_headless_final5_30ep_LLR')\n","model.save('/content/gdrive/MyDrive/content/models/mobilenetv2_all_combined_headless_final5_30ep_LLR.h5')"],"metadata":{"id":"gUPVHB2vdJjm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","plt.figure(figsize=(10, 10))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.ylabel('Accuracy')\n","plt.ylim([min(plt.ylim()),1])\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Cross Entropy')\n","plt.ylim([0,1.0])\n","plt.title('Training and Validation Loss')\n","plt.xlabel('epoch')\n","plt.show()"],"metadata":{"id":"XMVnXrILFvXQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!kill 2933\n","#! rm -R /content/gdrive/MyDrive/content/img_dir/logs/all_combined/fit4/\n","! ls -lrt /content/gdrive/MyDrive/content/img_dir/logs/all_combined/fit6/"],"metadata":{"id":"jutQuf7JawFs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" #! kill -9 2453\n"," %reload_ext tensorboard"],"metadata":{"id":"bRsyNKtCbjcs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!kill 2893\n","#%reload_ext tensorboard\n","%tensorboard --logdir=\"/content/gdrive/MyDrive/content/img_dir/logs/all_combined/fit6\"\n","#%tensorboard --logdir=log_dest"],"metadata":{"id":"KNG8G6CNKY8a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Check the predictions**\n","\n","Obtain the ordered list of class names from the model predictions:"],"metadata":{"id":"yiyC1iK_MFbT"}},{"cell_type":"code","source":["predicted_batch = model.predict(image_batch)\n","predicted_id = tf.math.argmax(predicted_batch, axis=-1)\n","predicted_label_batch = class_names[predicted_id]\n","print(predicted_label_batch)"],"metadata":{"id":"HfqPa7-MMG7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#y_ = placeholder_for_labels # for eg: [1, 2, 4]\n","#y = mycnn(...) # for eg: [2, 2, 4]\n","y_ = predicted_label_batch\n","y = predicted_batch\n","\n","\n","\n","confusion = tf.confusion_matrix(labels=y_, predictions=y, num_classes=class_names)"],"metadata":{"id":"TLKFK2ghp1KA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#confusion_mtx = tf.math.confusion_matrix(predicted_id, predicted_batch)\n","#plt.figure(figsize=(10, 8))\n","#sns.heatmap(confusion_mtx,\n","#            xticklabels=commands,\n","#            yticklabels=commands,\n","#            annot=True, fmt='g')\n","#plt.xlabel('Prediction')\n","#plt.ylabel('Label')\n","#plt.show()"],"metadata":{"id":"mlJ_mnckLowt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot **MODEL PREDICTIONS**"],"metadata":{"id":"jDMWHxYEMMT7"}},{"cell_type":"code","source":["plt.figure(figsize=(14,13))\n","plt.subplots_adjust(hspace=0.5)\n","\n","for n in range(30):\n","  plt.subplot(6,5,n+1)\n","  plt.imshow(image_batch[n])\n","  plt.title(predicted_label_batch[n].title())\n","  plt.axis('off')\n","_ = plt.suptitle(\"Model predictions\")"],"metadata":{"id":"qknTlMC-MLU7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.ylabel('Accuracy')\n","plt.ylim([min(plt.ylim()),1])\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Cross Entropy')\n","plt.ylim([0,1.0])\n","plt.title('Training and Validation Loss')\n","plt.xlabel('epoch')\n","plt.show()"],"metadata":{"id":"AbeXWWq-pDRv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's take a look to see how many layers are in the base model\n","print(\"Number of layers in the base model: \", len(  model.layers))\n","model.summary()"],"metadata":{"id":"JxKW1Oycxks-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fine tuning"],"metadata":{"id":"g1M9J4fvyM_8"}},{"cell_type":"code","source":["fine_tune_epochs = 30\n","total_epochs =  NUM_EPOCHS + fine_tune_epochs\n","\n","history_finetune = model.fit(train_ds,\n","                         epochs=total_epochs,\n","                         validation_data=val_ds,\n","                         initial_epoch=history.epoch[-1],\n","                         callbacks=tensorboard_callback)"],"metadata":{"id":"cMseb_Bkx4zg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save('/content/gdrive/MyDrive/content/models/mobilenetv2_all_combined_headless_ftune_final5_LLR')\n","model.save('/content/gdrive/MyDrive/content/models/mobilenetv2_all_combined_headless_ftune_final5_LLR.h5')\n","\n","# Convert the model to Tensorflow Lite\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","tflite_model = converter.convert()\n","\n","# Save the model.\n","with open('/content/gdrive/MyDrive/content/models/mobilenetv2_all_combined_headless_ftune_final5_20ep_LLR.tflite', 'wb') as f:\n","  f.write(tflite_model)"],"metadata":{"id":"2FmSPq04wW3m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","plt.figure(figsize=(10, 10))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.ylabel('Accuracy')\n","plt.ylim([min(plt.ylim()),1])\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Cross Entropy')\n","plt.ylim([0,1.0])\n","plt.title('Training and Validation Loss')\n","plt.xlabel('epoch')\n","plt.show()"],"metadata":{"id":"o9Zy3DRIyvb0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir=\"/content/gdrive/MyDrive/content/img_dir/logs/all_combined/fit6\"\n","#%tensorboard --logdir=log_dest"],"metadata":{"id":"Cq2rO6YVy4Qn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Retrieve a batch of images from the test set\n","image_batch, label_batch = val_ds.as_numpy_iterator().next()\n","predictions = model.predict_on_batch(image_batch).flatten()\n","\n","# Apply a sigmoid since our model returns logits\n","#predictions = tf.nn.sigmoid(predictions)\n","#predictions = tf.where(predictions < 0.5, 0, 1)\n","\n","print('Predictions:\\n', predictions.numpy())\n","print('Labels:\\n', label_batch)\n","\n","plt.figure(figsize=(12, 12))\n","for i in range(9):\n","  ax = plt.subplot(3, 3, i + 1)\n","  plt.imshow(image_batch[i].astype(\"uint8\"))\n","  plt.title(class_names[predictions[i]])\n","  plt.axis(\"off\")"],"metadata":{"id":"lo5pfUrA2fUT","colab":{"base_uri":"https://localhost:8080/","height":240},"executionInfo":{"status":"error","timestamp":1653914486041,"user_tz":-120,"elapsed":741,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"cd527105-0426-4fb5-dc52-a0f7b829eee4"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-7f8b048ca781>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#predictions = tf.where(predictions < 0.5, 0, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predictions:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Labels:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"2tQSarXNwSFV"},"execution_count":null,"outputs":[]}]}