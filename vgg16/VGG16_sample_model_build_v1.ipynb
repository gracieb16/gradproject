{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGG16_sample_model_build_v1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOSmI4U1pYhdIjRsG66Fai3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Check GPU"],"metadata":{"id":"XIkB0i9LmTCm"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"AVrjJZ2gmXtO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651085662428,"user_tz":-120,"elapsed":10,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"39b6e86f-f95e-451a-ab81-6f067a0c8d6a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Apr 27 18:54:21 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["Load Google Drive "],"metadata":{"id":"m5w5xQ54meTz"}},{"cell_type":"code","source":["from google.colab import drive #Only if you are using Google Drive\n","drive.mount('/content/gdrive')\n","drive.mount(\"/content/gdrive\", force_remount=True)\n","%cd /content/gdrive/My\\ Drive/"],"metadata":{"id":"zIQm_fwomemb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651085691608,"user_tz":-120,"elapsed":22569,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"725206d6-3a3b-49f9-90f9-c12faa5867cd"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","Mounted at /content/gdrive\n","/content/gdrive/My Drive\n"]}]},{"cell_type":"markdown","source":["Location of spectograms for home location, small sample testing"],"metadata":{"id":"01xeFa4Zmla4"}},{"cell_type":"code","source":["cd loc_home/"],"metadata":{"id":"CpuDnvAPmlx7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651085691608,"user_tz":-120,"elapsed":6,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"6cef265a-cd96-4702-dfaa-cf23d77a35cd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/loc_home\n"]}]},{"cell_type":"markdown","source":[""],"metadata":{"id":"3ql2HmuRmmFb"}},{"cell_type":"code","source":["%tensorflow_version  1.x"],"metadata":{"id":"2jGYM6-jmmZH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651085692157,"user_tz":-120,"elapsed":553,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"01a8fae1-aa1d-4368-bf55-c31e94e8aad2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}]},{"cell_type":"markdown","source":["**Model**"],"metadata":{"id":"JbMpPeDJmz2Q"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from glob import glob\n","\n","training_dir = './dataset/train/' #Define the directory of each\n","validation_dir = './dataset/validation/'\n","test_dir = './dataset/test'\n","\n","folders = glob(training_dir + '/*')\n","num_classes = len(folders)\n","print ('Total Classes = ' + str(num_classes))"],"metadata":{"id":"Arx8sN3_m5Cf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651085693730,"user_tz":-120,"elapsed":1575,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"c3dbda1c-f248-436a-ca0d-7eb404e7a86f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Classes = 52\n"]}]},{"cell_type":"markdown","source":["Import Libraries"],"metadata":{"id":"rDpcsCKrm7aj"}},{"cell_type":"code","source":["# importing requried libraries\n","from keras import applications\n","from keras.models import Model\n","import keras \n","from keras.applications.vgg16 import VGG16 #Importing the VGG16 Model\n","from keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D,BatchNormalization\n","from keras.applications import VGG16, MobileNetV2\n","from keras.optimizers import SGD\n","import tensorflow as tf\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.applications.vgg16 import preprocess_input  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q1UZUp2FW8gX","executionInfo":{"status":"ok","timestamp":1651085698000,"user_tz":-120,"elapsed":2737,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"ed4c7dff-e0be-4a72-d43e-40818c6aeb75"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["Using TensorFlow backend.\n"]}]},{"cell_type":"markdown","source":["**Data pre-processing**"],"metadata":{"id":"Xsks4ejQXix4"}},{"cell_type":"code","source":["IMAGE_SIZE = [224,224]   \n","\n","# Use ImageDataGenerator to import data with labels easily into the model.\n","# We will use function to rescale, rotate, zoom, flip etc\n","# This class doesnâ€™t affect the data stored on the disk. \n","# This class alters the data on the go while passing it to the model.\n","\n","# we use the preprocess_input function from VGG16 to normalize the input data\n","#training_datagen = image_dataset_from_directory(training_dir, labels='inferred', color_mode='rgb',image_size=(224,224))\n","#training_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n","training_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3, rotation_range=40, \n","                                      width_shift_range=0.3, height_shift_range=0.3, \n","                                      shear_range=0.3, horizontal_flip=True, fill_mode=\"nearest\",\n","                                      preprocessing_function=preprocess_input)\n","\n","#validation_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n","validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n","\n","training_generator = training_datagen.flow_from_directory(training_dir, target_size = IMAGE_SIZE, batch_size = 255, class_mode = 'categorical') #In this attempt, I am increasing the batch size to 255 in both training and validation\n","validation_generator = validation_datagen.flow_from_directory(validation_dir, target_size = IMAGE_SIZE, batch_size = 255, class_mode = 'categorical')\n","training_generator.class_indices\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jwXoHXEfXPj7","executionInfo":{"status":"ok","timestamp":1651085733177,"user_tz":-120,"elapsed":28101,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"78c5058d-42cc-4930-a2ce-cc5c6f7858ab"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 4055 images belonging to 52 classes.\n","Found 1947 images belonging to 69 classes.\n"]},{"output_type":"execute_result","data":{"text/plain":["{'Alarm': 0,\n"," 'Applause': 1,\n"," 'Bathtub_filling_or_washing': 2,\n"," 'Bell': 3,\n"," 'Boiling': 4,\n"," 'Chatter': 5,\n"," 'Chink_and_clink': 6,\n"," 'Chirp_and_tweet': 7,\n"," 'Coin_dropping': 8,\n"," 'Computer_keyboard': 9,\n"," 'Cupboard_open_or_close': 10,\n"," 'Cutlery_and_silverware': 11,\n"," 'Dishes_and_pots_and_pans': 12,\n"," 'Domestic_sounds_and_home_sounds': 13,\n"," 'Door': 14,\n"," 'Doorbell': 15,\n"," 'Drawer_open_or_close': 16,\n"," 'Drip': 17,\n"," 'Engine': 18,\n"," 'Fart': 19,\n"," 'Fill_with_liquid': 20,\n"," 'Frying_food': 21,\n"," 'Idling': 22,\n"," 'Keys_jangling': 23,\n"," 'Knock': 24,\n"," 'Male_speech_and_man_speaking': 25,\n"," 'Meow': 26,\n"," 'Microwave_oven': 27,\n"," 'Packing_tape_and_duct_tape': 28,\n"," 'Raindrop': 29,\n"," 'Rattle': 30,\n"," 'Run': 31,\n"," 'Scissors': 32,\n"," 'Screech': 33,\n"," 'Sink_filling_or_washing': 34,\n"," 'Slam': 35,\n"," 'Sliding_door': 36,\n"," 'Squeak': 37,\n"," 'Subway_and_metro_and_underground': 38,\n"," 'Tap': 39,\n"," 'Tearing': 40,\n"," 'Telephone': 41,\n"," 'Toilet_flush': 42,\n"," 'Tools': 43,\n"," 'Trickle_and_dribble': 44,\n"," 'Typewriter': 45,\n"," 'Typing': 46,\n"," 'Walk_and_footsteps': 47,\n"," 'Water_tap_and_faucet': 48,\n"," 'Wood': 49,\n"," 'Writing': 50,\n"," 'Zipper_clothing': 51}"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["\n","#Loading VGG16 Model\n","IMAGE_SIZE = [224,224]   \n","\n","#we use the Include_top=False to remove the classification \n","#layer that was trained on the ImageNet dataset and set the model as not trainable\n","vgg_model = VGG16(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False)  \n","\n","for layer in vgg_model.layers:\n","      layer.trainable = False # Non trainable weights\n","\n","# Create Dense Layers\n","# Add the last layers (Flatten and Dense layers) for our problem\n","x = vgg_model.output\n","x = Flatten()(x) \n","# After creating all the convolution, pass the data to the dense layer to flatten the vector \n","# which comes out of the convolutions (Refer to VGG16 image dimention layers above)\n","x = Dense(num_classes, activation = 'softmax')(x)\n","x = Dense(69, activation = 'softmax')(x) \n","#x = Dense(69, activation = 'softmax')(x) \n","x = Dropout(0.5)(x) # Dropout layer to reduce overfitting\n","transfer_model = Model(inputs = vgg_model.input, outputs = x)\n","# Compile model, for this we will be using ADAM optimiser to reach to the global minima while training our model\n","learning_rate= 5e-5\n","transfer_model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n","#history = transfer_model.fit(X_train, y_train, batch_size = 1, epochs=50, validation_data=(X_test,y_test))\n","\n","#transfer_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n","transfer_model.summary() #check summary of the model using this command\n"],"metadata":{"id":"WsCCADhNnF-a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651085741186,"user_tz":-120,"elapsed":8029,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"30af8a3b-5102-409b-c531-02c108c34775"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 3s 0us/step\n","Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 25088)             0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 52)                1304628   \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 69)                3657      \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 69)                0         \n","=================================================================\n","Total params: 16,022,973\n","Trainable params: 1,308,285\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":[""],"metadata":{"id":"EGn7Qp1xnOZB"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"ziJaGIWPnTNh"}},{"cell_type":"code","source":["history = transfer_model.fit_generator(training_generator,\n","                   steps_per_epoch = 16, #Number of iterations = number of training images (4055) / batch size (255)  \n","                   epochs = 10, \n","                   validation_data = validation_generator,\n","                   validation_steps = 2, #same for validation data 300 validation images\n","                   shuffle = True) \n","model.save('AED_A_100_vgg16_home_255_Param.h5')"],"metadata":{"id":"Qjlnlpz4nTnB","colab":{"base_uri":"https://localhost:8080/","height":534},"executionInfo":{"status":"error","timestamp":1651085956926,"user_tz":-120,"elapsed":198768,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"7bef32b9-92a1-499a-a759-d4bd5b45e6cd"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/tensorflow-1.15.2/python3.7/keras/utils/data_utils.py:616: UserWarning: The input 7 could not be retrieved. It could be because a worker has died.\n","  UserWarning)\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-d3147f1e9dee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#same for validation data 300 validation images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                    shuffle = True) \n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AED_A_100_vgg16_home_255_Param.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1506\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dropout_1 to have shape (69,) but got array with shape (52,)"]}]},{"cell_type":"markdown","source":[""],"metadata":{"id":"3Uy4h22anX9o"}},{"cell_type":"code","source":["from keras.models import load_model\n","model = load_model('A_100_vgg16_v10e_255_Param.h5')"],"metadata":{"id":"ddbxHXHPnYNb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"31ovqz73ndSS"}},{"cell_type":"code","source":["!pip install 'h5py==2.10.0' --force-reinstall #Sometimes the above makes error, use this command only if the above made an error"],"metadata":{"id":"T4WujhjYndba"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Model Testing"],"metadata":{"id":"rXKjbMfLneyQ"}},{"cell_type":"code","source":["from keras.preprocessing.image import array_to_img, img_to_array, load_img\n","from keras.applications.vgg16 import preprocess_input\n","import numpy as np\n","import os\n","img_size = 224\n","clssN = ['Heidelberglaan15','Padualaan101','Padualaan97']\n","count = 0\n","countT = 0\n","for bl in clssN:\n","  src = './dataset2/Test/' + bl \n","  imgs = os.listdir(src)\n","  count = 0\n","  countT = len(imgs) #total number of images per class\n","  for img in imgs: \n","    im = load_img(src + \"/\" + img)\n","    w,h = im.size\n","    im = im.resize((int(w*0.2),int(h*0.2)))\n","    im = im.resize((img_size,img_size))\n","    x = img_to_array(im)  \n","    x = np.expand_dims(x, axis=0)\n","    x = preprocess_input(x)\n","    x/=255.\n","    probs = model.predict(x, verbose=0)\n","    maxDet = max(probs[0])\n","    maxIdx = list(probs[0]).index(maxDet) #predicted class with maximum probability\n","    if (clssN[maxIdx]==bl): \n","      count = count + 1  #count for true predictions (true positives)\n","    else:\n","      print(f'Wrong prediction of {bl}  {img}. Predicted as {clssN[maxIdx]}') \n","  print(f'test accuracy for {bl} is {str(count/countT)}')"],"metadata":{"id":"_3UVqDSlnfAq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using the below code, you will see the output of what building class the model has predicted for this specific image."],"metadata":{"id":"v3oraEgunsE3"}},{"cell_type":"code","source":["from keras.preprocessing.image import array_to_img, img_to_array, load_img\n","from keras.applications.vgg16 import preprocess_input\n","import numpy as np\n","import os\n","img_size = 224\n","clssN = ['Heidelberglaan15','Padualaan101','Padualaan97']\n","imgDir = \"dataset2/Test/Padualaan101/20211220_121005.jpg\"\n","im = load_img(imgDir)\n","w,h = im.size\n","im = im.resize((int(w*0.2),int(h*0.2)))\n","im = im.resize((img_size,img_size))\n","x = img_to_array(im)  \n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","x/=255.\n","probs = model.predict(x, verbose=0)\n","maxDet = max(probs[0])\n","maxIdx = list(probs[0]).index(maxDet) #predicted class with maximum probability\n","print(clssN[maxIdx])"],"metadata":{"id":"3zqfsMuXntC4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plotting loss and accuracy"],"metadata":{"id":"P4HbbDUH8gRH"}},{"cell_type":"code","source":["from matplotlib import pyplot\n","# plot loss during training\n","pyplot.subplot(211)\n","pyplot.title('Loss')\n","pyplot.plot(history.history['loss'], label='train')\n","pyplot.plot(history.history['val_loss'], label='test')\n","pyplot.legend()\n","# plot accuracy during training\n","pyplot.subplot(212)\n","pyplot.title('Accuracy')\n","pyplot.plot(history.history['accuracy'], label='train')\n","pyplot.plot(history.history['val_accuracy'], label='test')\n","pyplot.legend()\n","pyplot.show()"],"metadata":{"id":"fqfPG5md8iZp"},"execution_count":null,"outputs":[]}]}