{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"vgg16_tensorflow_variant_testing.ipynb","provenance":[],"machine_shape":"hm","background_execution":"on","collapsed_sections":[],"authorship_tag":"ABX9TyOKyywYcwzxSIA28T+DT3SL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QvdbkHKFVQlh","executionInfo":{"status":"ok","timestamp":1653786508695,"user_tz":-120,"elapsed":21,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"9d84a292-43dd-404a-da5a-b0c66220b7b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sun May 29 01:08:28 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P0    25W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","source":["from google.colab import drive #Only if you are using Google Drive\n","drive.mount('/content/gdrive')\n","drive.mount(\"/content/gdrive\", force_remount=True)\n","%cd /content/gdrive/My\\ Drive/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DIBl5lzLVS__","executionInfo":{"status":"ok","timestamp":1653786531214,"user_tz":-120,"elapsed":22534,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"841840a7-1c89-4384-ca8b-bd45ac8e2c14"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","Mounted at /content/gdrive\n","/content/gdrive/My Drive\n"]}]},{"cell_type":"code","source":["cd /content/gdrive/MyDrive/content/img_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6VKsOGp-VTCZ","executionInfo":{"status":"ok","timestamp":1653786531654,"user_tz":-120,"elapsed":463,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"c7fb3358-0212-43c6-ef82-06cac1fae189"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/content/img_dir\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from glob import glob\n","\n","from keras import applications\n","from keras.models import Model\n","import keras \n","from keras.applications.vgg16 import VGG16 #Importing the VGG16 Model\n","from keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D,BatchNormalization\n","#from keras.applications import MobileNetV2\n","#from keras.optimizers import SGD\n","import tensorflow as tf\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.applications.vgg16 import preprocess_input  "],"metadata":{"id":"hzi91RxMVTEr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_dir = '/content/gdrive/MyDrive/content/img_dir/train'\n","validation_dir = '/content/gdrive/MyDrive/content/img_dir/test'\n","test_dir = '/content/gdrive/MyDrive/content/img_dir/test'\n","\n","folders = glob(training_dir + '/*')\n","num_classes = len(folders)\n","print ('Total Classes = ' + str(num_classes))\n","print('Total training classes '+str(len(glob(training_dir + '/*'))))\n","print('Total test classes '+str(len(glob(test_dir + '/*'))))\n","print('Toral validation classes '+str(len(glob(validation_dir + '/*'))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"siQgE6CwVTHL","executionInfo":{"status":"ok","timestamp":1653786534398,"user_tz":-120,"elapsed":29,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"40676a93-c481-4cd0-a2f0-6046dc20b61f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Classes = 10\n","Total training classes 10\n","Total test classes 10\n","Toral validation classes 10\n"]}]},{"cell_type":"markdown","source":["Data option 1"],"metadata":{"id":"6bKB2jCkWLpR"}},{"cell_type":"code","source":["IMAGE_SIZE = [224,224]\n","BATCH_SIZE = 32\n","\n","training_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,samplewise_center=True, samplewise_std_normalization=True)\n","training_generator = training_datagen.flow_from_directory(directory=training_dir,target_size = IMAGE_SIZE, batch_size = BATCH_SIZE, class_mode = 'categorical')\n","validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, samplewise_center=True, samplewise_std_normalization=True)\n","validation_generator = validation_datagen.flow_from_directory(directory=test_dir, target_size = IMAGE_SIZE, batch_size = BATCH_SIZE, class_mode = 'categorical')\n","training_generator.class_indices\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aoxznOx8VTJc","executionInfo":{"status":"ok","timestamp":1653786686258,"user_tz":-120,"elapsed":620,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"023b84bd-31b9-48a4-b09f-d4da649f6095"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 6969 images belonging to 10 classes.\n","Found 1747 images belonging to 10 classes.\n"]},{"output_type":"execute_result","data":{"text/plain":["{'air_conditioner': 0,\n"," 'car_horn': 1,\n"," 'children_playing': 2,\n"," 'dog_bark': 3,\n"," 'drilling': 4,\n"," 'engine_idling': 5,\n"," 'gun_shot': 6,\n"," 'jackhammer': 7,\n"," 'siren': 8,\n"," 'street_music': 9}"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["Data Option 2"],"metadata":{"id":"n2MXlP47WONW"}},{"cell_type":"code","source":["# 255 batch size\n","IMAGE_SIZE = [224,224]\n","BATCH_SIZE = 32\n","\n","training_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3, rotation_range=40, \n","                                      width_shift_range=0.3, height_shift_range=0.3, \n","                                      shear_range=0.3, horizontal_flip=True, fill_mode=\"nearest\",\n","                                      preprocessing_function=preprocess_input,\n","                                      samplewise_center=True, samplewise_std_normalization=True)\n","training_generator = training_datagen.flow_from_directory(directory=training_dir,target_size = IMAGE_SIZE, batch_size = BATCH_SIZE, class_mode = 'categorical')\n","validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, samplewise_center=True, samplewise_std_normalization=True)\n","validation_generator = validation_datagen.flow_from_directory(directory=test_dir, target_size = IMAGE_SIZE, batch_size = BATCH_SIZE, class_mode = 'categorical')\n","training_generator.class_indices"],"metadata":{"id":"hBbkXyruVTL4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vgg_model = VGG16(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False)  \n","\n","for layer in vgg_model.layers:\n","      layer.trainable = False # Non trainable weights\n","\n","# Create Dense Layers\n","# Add the last layers (Flatten and Dense layers) for our problem\n","x = Flatten()(vgg_model.output) \n","x = Dense(num_classes, activation = 'softmax')(x)\n","x = Dropout(0.5)(x) # Dropout layer to reduce overfitting\n","\n","transfer_model = Model(inputs = vgg_model.input, outputs = x)\n","# Compile model, for this we will be using ADAM optimiser to reach to the global minima while training our model\n","# learning_rate= 5e-5\n","#transfer_model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n","transfer_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n","                       optimizer=tf.keras.optimizers.Adam(1e-3),\n","                       metrics=['accuracy'])\n","\n","#transfer_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","#                       optimizer=tf.keras.optimizers.Adam(1e-3),\n","#                       metrics=['accuracy'])\n","#history = transfer_model.fit(X_train, y_train, batch_size = 1, epochs=50, validation_data=(X_test,y_test))\n","\n","#transfer_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n","transfer_model.summary() #check summary of the model using this command\n","\n","\n","  #optimizer=tf.keras.optimizers.Adam(1e-3),\n","  #loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FVx8lFuoWYo5","executionInfo":{"status":"ok","timestamp":1653786931566,"user_tz":-120,"elapsed":1091,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"3225c5b2-ef28-474c-e80e-a341e5d99bda"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 25088)             0         \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                250890    \n","                                                                 \n"," dropout_1 (Dropout)         (None, 10)                0         \n","                                                                 \n","=================================================================\n","Total params: 14,965,578\n","Trainable params: 250,890\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# 8716 images, 32 batch size \n","history = transfer_model.fit(training_generator,\n","                   steps_per_epoch =272, #Number of iterations = number of training images (49,225) / batch size (255)  \n","                   epochs = 10, \n","                   validation_data = validation_generator,\n","                   validation_steps = 2, #same for validation data 300 validation images\n","                   shuffle = True) \n","transfer_model.save('/content/gdrive/MyDrive/final/AEC_vgg16_us8k_272steps_10epochs_Param.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bWfPtcmOWYrd","executionInfo":{"status":"ok","timestamp":1653789052370,"user_tz":-120,"elapsed":2072091,"user":{"displayName":"Gracie G. Becongco","userId":"00295426205393146083"}},"outputId":"ba01db61-4fc9-4eaf-dc06-c7c5b4da969a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","218/272 [=======================>......] - ETA: 8:22 - loss: nan - accuracy: 0.1725WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2720 batches). You may need to use the repeat() function when building your dataset.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2720 batches). You may need to use the repeat() function when building your dataset.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r272/272 [==============================] - 2071s 8s/step - loss: nan - accuracy: 0.1725 - val_loss: nan - val_accuracy: 0.0938\n"]}]},{"cell_type":"code","source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.ylabel('Accuracy')\n","plt.ylim([min(plt.ylim()),1])\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Cross Entropy')\n","plt.ylim([0,1.0])\n","plt.title('Training and Validation Loss')\n","plt.xlabel('epoch')\n","plt.show()"],"metadata":{"id":"9LVh19tdWYwk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"dN61gKPhWYzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"jSpVew_JWY1t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tensorflow variant of VGG16"],"metadata":{"id":"CYBEJmeXZx9F"}},{"cell_type":"code","source":[""],"metadata":{"id":"LhO3DVRQZ1vW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","img_height = 224\n","img_width = 224\n","#BATCH_SIZE = 255\n","IMG_SIZE = (224, 224)\n","\n","#train_dir = '/content/gdrive/MyDrive/content/img_dir/combined_data/train'\n","#validation_dir = '/content/gdrive/MyDrive/content/img_dir/combined_data/test/'\n","data_dir = '/content/gdrive/MyDrive/content/img_dir/combined_data/all_labeled/'\n","\n","vgg16 = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet')\n","\n","\n","feature_extractor_model = vgg16"],"metadata":{"id":"8I8QTfRdZ1yI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds = tf.keras.utils.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)\n","\n","val_ds = tf.keras.utils.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)\n","\n","data_augmentation = tf.keras.Sequential([\n","  tf.keras.layers.RandomFlip('horizontal'),\n","  tf.keras.layers.RandomRotation(0.2),\n","])\n","\n","class_names = np.array(train_ds.class_names)\n","print(class_names)"],"metadata":{"id":"DH8FgG3xZ10Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["normalization_layer = tf.keras.layers.Rescaling(1./255)\n","train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n","val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n","\n","AUTOTUNE = tf.data.AUTOTUNE\n","train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","for image_batch, labels_batch in train_ds:\n","  print(image_batch.shape)\n","  print(labels_batch.shape)\n","  break"],"metadata":{"id":"27N0N91IZ13F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_extractor_layer = hub.KerasLayer(\n","    feature_extractor_model,\n","    input_shape=(224, 224, 3),\n","    trainable=False)"],"metadata":{"id":"xuleoIIqaccB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_batch = feature_extractor_layer(image_batch)\n","print(feature_batch.shape)"],"metadata":{"id":"mCDqVT3MacnL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_classes = len(class_names)\n","\n","model = tf.keras.Sequential([\n","  feature_extractor_layer,\n","  tf.keras.layers.Dense(num_classes)\n","])\n","\n","model.summary()"],"metadata":{"id":"zFokK9dZbJdf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(\n","  optimizer=tf.keras.optimizers.Adam(1e-3),\n","  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","  metrics=['acc'])\n","\n","log = \"/content/gdrive/MyDrive/content/img_dir/logs/vgg16/all_combined/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log, histogram_freq=1) # Enable histogram computation for every epoch."],"metadata":{"id":"OgGizgqRbJa3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_EPOCHS = 40\n","\n","history = model.fit(train_ds,\n","                    validation_data=val_ds,\n","                    epochs=NUM_EPOCHS,\n","                    callbacks=tensorboard_callback)"],"metadata":{"id":"HADdaytZbJX2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save('/content/gdrive/MyDrive/content/vgg16_all_combined_headless_final')\n","model.save('/content/gdrive/MyDrive/content/vgg16_all_combined_headless_final.h5')"],"metadata":{"id":"z9v-KjOTbJQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.ylabel('Accuracy')\n","plt.ylim([min(plt.ylim()),1])\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Cross Entropy')\n","plt.ylim([0,1.0])\n","plt.title('Training and Validation Loss')\n","plt.xlabel('epoch')\n","plt.show()"],"metadata":{"id":"FqpSiqOHacqj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" %reload_ext tensorboard"],"metadata":{"id":"If37keMcbxDb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir=\"/content/gdrive/MyDrive/content/img_dir/logs/vgg16/all_combined/fit/\""],"metadata":{"id":"amHqQeUzbxOs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Ss-09lD1bxeL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.keras.applications.vgg16.VGG16(\n","    include_top=True,\n","    weights='imagenet',\n","    input_tensor=None,\n","    input_shape=None,\n","    pooling=None,\n","    classes=1000,\n","    classifier_activation='softmax'\n",")"],"metadata":{"id":"yZ04m8-6Z15t"},"execution_count":null,"outputs":[]}]}